{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "Use historical boxscores to predict an NBA player's Ownership Percentage, or the percent of lineups in a contest that include the player. I will only project a player's ownerhship percentage for GPP contests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "This projection will help evaluate players when building lineups to enter DFS contests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (features.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/brandonshimiaie/Projects/pyNBA/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3437\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-5fa748a1e588>\"\u001b[0m, line \u001b[1;32m6\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from pyNBA.Models.helpers import CleanData\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/brandonshimiaie/Projects/pyNBA/pyNBA/Models/helpers.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pyNBA.Models.features import FeatureCreation\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/brandonshimiaie/Projects/pyNBA/pyNBA/Models/features.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    teimport pandas as pd\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyNBA.Data.data import QueryData\n",
    "from pyNBA.Models.helpers import CleanData\n",
    "import math\n",
    "from pyNBA.Data.constants import BAD_CONTEST_SUBSTRINGS, DB_TEAM_TO_NBA_TEAM, OWNERSHIP_NAME_TO_NBA_NAME\n",
    "from functools import reduce\n",
    "\n",
    "# for features\n",
    "from pyNBA.Models.features import FeatureCreation\n",
    "from pyNBA.Models.cluster import Cluster, Evaluate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from pyNBA.DFS.rules import FPCalculator\n",
    "from pyNBA.DFS.constants import Site\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from research import Helpers\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "\n",
    "# for statistical tests\n",
    "from scipy.stats import shapiro\n",
    "import pingouin as pg\n",
    "\n",
    "# for machine learning\n",
    "from sklearn import model_selection, preprocessing, ensemble, neighbors, linear_model, svm, neural_network, metrics\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for explainer\n",
    "from lime import lime_tabular\n",
    "\n",
    "# misc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Determination\n",
    "Determine the factors that influence a player's Ownership Percentage.\n",
    "\n",
    "- Player Data\n",
    "    - Fantasy production\n",
    "    - Value\n",
    "    - Recent value\n",
    "    - Salary\n",
    "    - Salary change\n",
    "    - Starter\n",
    "    - Variance\n",
    "    - Positions\n",
    "    - Number of positions\n",
    "    - Average Ownership\n",
    "    - Recent Ownership\n",
    "    - Number of positions of player\n",
    "    - Positions of player (binary)\n",
    "\n",
    "\n",
    "- Defense\n",
    "    - DvP\n",
    "    - Vegas total\n",
    "    - Vegas point spread\n",
    "    \n",
    "\n",
    "- Slate\n",
    "    - Number of games in slate\n",
    "    - Number of players in position\n",
    "    - Player position value rank\n",
    "    - Number of players close to salary in position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Pull all necessary data, and prepare it for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will pull all necessary historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query_data = QueryData()\n",
    "clean_data = CleanData()\n",
    "\n",
    "# pull boxscore data\n",
    "boxscores = query_data.query_boxscore_data()\n",
    "\n",
    "# we are only interested in the boxscores of healthy and non-resting players\n",
    "boxscores = clean_data.drop_rows_player_injured(boxscores)\n",
    "boxscores = clean_data.drop_rows_player_rest(boxscores)\n",
    "\n",
    "# historical DFS salary data\n",
    "salary_data = query_data.query_salary_data()\n",
    "salary_data = salary_data.rename(columns={\"POSITION\": \"DFS_POSITION\"})\n",
    "\n",
    "# historical DFS contest data. I am only interested in predicting ownership in Classic GPP compeititions.\n",
    "contest_data = query_data.query_contest_data()\n",
    "contest_data = contest_data.loc[\n",
    "    (contest_data['CASHLINE'] > 0) & (contest_data['SLATETYPE'] == 'Classic') & (contest_data['CASHLINE'] > 200) &\n",
    "    (~contest_data['CONTESTNAME'].str.lower().str.contains('|'.join(BAD_CONTEST_SUBSTRINGS)))\n",
    "].dropna(subset=['CASHLINE'])\n",
    "contest_data['MAXROI'] = contest_data['TOPPRIZE']/contest_data['ENTRYFEE']\n",
    "contest_data = contest_data.loc[contest_data['MAXROI'] > 2]\n",
    "\n",
    "# historical player ownership\n",
    "ownership_data = query_data.query_ownership_data()\n",
    "\n",
    "# historical vegas odds data\n",
    "odds_data = query_data.query_odds_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to standardize a player's name, to join the salaries and boxscores tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data['NAME'] = salary_data['PLAYER'].apply(clean_data.convert_rotoguru_name_to_nba_name)\n",
    "salary_data = salary_data.loc[salary_data['SITE'] == Site.DRAFTKINGS]\n",
    "boxscores = boxscores.merge(salary_data, how='inner', on=['DATE', 'NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create new features from raw data using domain knowlege."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'OWNERSHIP'\n",
    "regressors = []\n",
    "categorical_regressors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Production\n",
    "First, I will create features that represent a player's production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DKFPCalculator = FPCalculator(Site.DRAFTKINGS)\n",
    "\n",
    "boxscores['REB'] = boxscores['DREB'] + boxscores['OREB']\n",
    "boxscores['DKFP'] = boxscores.apply(\n",
    "    lambda x: DKFPCalculator.calculate_draftkings_fp(\n",
    "        x['PTS'], x['REB'], x['AST'], x['TOV'], x['BLK'], x['STL'], x['FG3M']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "feature_creation = FeatureCreation()\n",
    "\n",
    "boxscores = feature_creation.expanding_mean(\n",
    "    df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='DKFP', new_col_name='AVG_DKFP'\n",
    ")\n",
    "regressors.append('AVG_DKFP')\n",
    "\n",
    "boxscores['VALUE'] = boxscores['AVG_DKFP']/boxscores['SALARY']\n",
    "regressors.append('VALUE')\n",
    "\n",
    "boxscores = feature_creation.lag(\n",
    "    df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='DKFP', new_col_name='L1_DKFP', n_shift=1\n",
    ")\n",
    "regressors.append('L1_DKFP')\n",
    "\n",
    "boxscores = feature_creation.rolling_mean(\n",
    "    df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='DKFP', new_col_name='MA5_DKFP', n_rolling=5\n",
    ")\n",
    "regressors.append('MA5_DKFP')\n",
    "\n",
    "boxscores = feature_creation.lag(\n",
    "    df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='SALARY', new_col_name='L1_SALARY', n_shift=1\n",
    ")\n",
    "boxscores['SALARY_CHANGE'] = boxscores['SALARY'] - boxscores['L1_SALARY']\n",
    "regressors.append('SALARY')\n",
    "regressors.append('SALARY_CHANGE')\n",
    "\n",
    "boxscores = feature_creation.expanding_standard_deviation(\n",
    "    df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='DKFP', new_col_name='STD_DKFP', min_periods=5\n",
    ")\n",
    "regressors.append('STD_DKFP')\n",
    "\n",
    "regressors.append('START')\n",
    "\n",
    "boxscores['DFS_POSITIONS'] = boxscores['DFS_POSITION'].apply(lambda x: x.split('_') if isinstance(x, str) else np.nan)\n",
    "boxscores['NUM_POSITIONS'] = boxscores['DFS_POSITIONS'].apply(lambda x: len(x) if isinstance(x, list) else np.nan)\n",
    "regressors.append('NUM_POSITIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positions = boxscores['DFS_POSITIONS'].dropna().explode().unique()\n",
    "for position in positions:\n",
    "    boxscores[position] = 0\n",
    "    boxscores.loc[~(boxscores['DFS_POSITION'].isnull()) & (boxscores['DFS_POSITION'].str.contains(position)), position] = 1\n",
    "    regressors.append(position)\n",
    "    categorical_regressors.append(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ownership_data['NAME'] = ownership_data['PLAYERNAME'].apply(lambda x: x if x not in OWNERSHIP_NAME_TO_NBA_NAME else OWNERSHIP_NAME_TO_NBA_NAME[x])\n",
    "ownership_data = ownership_data.merge(contest_data, on=['SLATEID', 'CONTESTNAME'], how='inner')\n",
    "ownership_data = ownership_data.groupby(['DATE', 'SLATEID', 'GAMECOUNT', 'NAME']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'OWNERSHIP': (x['OWNERSHIP']*x['TOTALENTRIES']).sum()/x['TOTALENTRIES'].sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "aggregated_ownership = ownership_data.groupby(['DATE', 'NAME']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'TOTAL_OWNERSHIP': x['OWNERSHIP'].mean()\n",
    "    })\n",
    ").reset_index()\n",
    "boxscores = boxscores.merge(aggregated_ownership, on=['DATE', 'NAME'], how='inner')\n",
    "\n",
    "boxscores = feature_creation.expanding_mean(\n",
    "    df=boxscores, group_col_names=['SEASON', 'NAME'], col_name='TOTAL_OWNERSHIP', new_col_name='AVG_OWNERSHIP'\n",
    ")\n",
    "regressors.append('AVG_OWNERSHIP')\n",
    "\n",
    "boxscores = feature_creation.lag(\n",
    "    df=boxscores, group_col_names=['SEASON', 'NAME'], col_name='TOTAL_OWNERSHIP', new_col_name='L1_OWNERSHIP', n_shift=1\n",
    ")\n",
    "regressors.append('L1_OWNERSHIP')\n",
    "\n",
    "boxscores = feature_creation.rolling_mean(\n",
    "    df=boxscores, group_col_names=['SEASON', 'NAME'], col_name='TOTAL_OWNERSHIP', new_col_name='MA5_OWNERSHIP', n_rolling=5\n",
    ")\n",
    "regressors.append('MA5_OWNERSHIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense\n",
    "I will create features that describe how favorable a matchup is, from a fantasy point production standpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DvP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxscores['NORM_POS'] = boxscores['POSITION'].apply(lambda x: x if '-' not in x else x.split('-')[0])\n",
    "\n",
    "temp = boxscores.dropna(subset=['DKFP', 'AVG_DKFP'])\n",
    "grouped_defensive_boxscores = temp.groupby(['SEASON', 'DATE', 'NORM_POS', 'OPP_TEAM']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'TEAM_DKFP_ALLOWED_P': x['DKFP'].sum(),\n",
    "        'TEAM_DKFP_AVG_P': x['AVG_DKFP'].sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "grouped_defensive_boxscores['DvP'] = grouped_defensive_boxscores['TEAM_DKFP_ALLOWED_P'] - grouped_defensive_boxscores['TEAM_DKFP_AVG_P']\n",
    "\n",
    "grouped_defensive_boxscores = feature_creation.expanding_mean(\n",
    "    df=grouped_defensive_boxscores, group_col_names=['SEASON', 'OPP_TEAM', 'NORM_POS'], col_name='DvP',\n",
    "    new_col_name='AVG_DvP', order_idx_name='DATE', min_periods=5\n",
    ")\n",
    "regressors.append('AVG_DvP')\n",
    "\n",
    "boxscores = boxscores.merge(grouped_defensive_boxscores, on=['SEASON', 'DATE', 'OPP_TEAM', 'NORM_POS'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vegas Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_data['TOTAL'] = odds_data['TOTAL'].replace(['PK', '-'], np.nan)\n",
    "odds_data['POINTSPREAD'] = odds_data['POINTSPREAD'].replace(['PK', '-'], 0)\n",
    "full_game_odds = odds_data.loc[odds_data['PERIOD'] == 'Full Game']\n",
    "boxscores = boxscores.merge(full_game_odds, on=['DATE', 'TEAM'], how='left')\n",
    "regressors.append('TOTAL')\n",
    "regressors.append('POINTSPREAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slate Info\n",
    "Lastly, I will include features that capture the slate's depth of each position, salary range, and how each player's expeted value compares to other players in the same position/salary range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors.append('GAMECOUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slates = contest_data.loc[contest_data['SITE'] == Site.DRAFTKINGS, ['DATE', 'SLATEID', 'TEAMS']].drop_duplicates()\n",
    "slates['TEAMS'] = slates['TEAMS'].apply(lambda x: x.split('_'))\n",
    "slates = slates.explode('TEAMS').rename(columns={\"TEAMS\": \"TEAM\"})\n",
    "slates['TEAM'] = slates['TEAM'].apply(lambda x: x if x not in DB_TEAM_TO_NBA_TEAM else DB_TEAM_TO_NBA_TEAM[x])\n",
    "\n",
    "slate_players = boxscores[['DATE', 'TEAM', 'NAME', 'DFS_POSITIONS', 'SALARY', 'VALUE']].merge(\n",
    "    slates, on=['DATE', 'TEAM'], how='inner'\n",
    "    )\n",
    "slate_players['SALARY_BIN'] = pd.cut(\n",
    "    slate_players['SALARY'], bins=list(range(3000, 15000, 1000)), duplicates='drop', include_lowest=True\n",
    "    )\n",
    "slate_players = slate_players.explode('DFS_POSITIONS').rename(columns={'DFS_POSITIONS': 'SINGLE_DFS_POSITION'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_VALUE = 0.0025\n",
    "\n",
    "all_temp = slate_players.groupby(['SLATEID', 'SINGLE_DFS_POSITION']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L1P_COUNT': x.loc[x['VALUE'] > MIN_VALUE, 'NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(all_temp, on=['SLATEID', 'SINGLE_DFS_POSITION'], how='left')\n",
    "\n",
    "sb_temp = slate_players.groupby(['SLATEID', 'SINGLE_DFS_POSITION', 'SALARY_BIN']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L1P_SB_COUNT': x['NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(sb_temp, on=['SLATEID', 'SINGLE_DFS_POSITION', 'SALARY_BIN'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_TO_L2 = {'PG': 'G', 'SG': 'G', 'SF': 'F', 'PF': 'F', 'C': 'C'}\n",
    "slate_players['LEVEL2_DFS_POSITION'] = slate_players['SINGLE_DFS_POSITION'].apply(lambda x: L1_TO_L2[x] if isinstance(x, str) else np.nan)\n",
    "\n",
    "all_temp = slate_players.groupby(['SLATEID', 'LEVEL2_DFS_POSITION']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L2P_COUNT': x.loc[x['VALUE'] > MIN_VALUE, 'NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(all_temp, on=['SLATEID', 'LEVEL2_DFS_POSITION'], how='left')\n",
    "\n",
    "sb_temp = slate_players.groupby(['SLATEID', 'LEVEL2_DFS_POSITION', 'SALARY_BIN']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L2P_SB_COUNT': x['NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(sb_temp, on=['SLATEID', 'LEVEL2_DFS_POSITION', 'SALARY_BIN'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_temp = slate_players.groupby(['SLATEID']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L3P_COUNT': x.loc[x['VALUE'] > MIN_VALUE, 'NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(all_temp, on=['SLATEID'], how='left')\n",
    "\n",
    "sb_temp = slate_players.groupby(['SLATEID', 'SALARY_BIN']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L3P_SB_COUNT': x.loc[x['VALUE'] > MIN_VALUE, 'NAME'].count()\n",
    "    })\n",
    ").reset_index().dropna()\n",
    "slate_players = slate_players.merge(sb_temp, on=['SLATEID', 'SALARY_BIN'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slate_players['SALARY_FLOOR'] = slate_players['SALARY_BIN'].apply(lambda x: x.left)\n",
    "\n",
    "slate_players['L1P_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID', 'SINGLE_DFS_POSITION']\n",
    ")['VALUE'].rank(method='min', ascending=False)\n",
    "\n",
    "slate_players['L1P_SB_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID', 'SINGLE_DFS_POSITION', 'SALARY_FLOOR']\n",
    ")['VALUE'].rank(method='min', ascending=False)\n",
    "\n",
    "slate_players['L2P_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID', 'LEVEL2_DFS_POSITION']\n",
    ")['VALUE'].rank(method='min', ascending=False)\n",
    "\n",
    "slate_players['L2P_SB_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID', 'LEVEL2_DFS_POSITION', 'SALARY_FLOOR']\n",
    ")['VALUE'].rank(method='min', ascending=False)\n",
    "\n",
    "slate_players['L3P_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID']\n",
    ")['VALUE'].rank(method='min', ascending=False)\n",
    "\n",
    "slate_players['L3P_SB_RANK'] = slate_players.groupby(\n",
    "    ['SLATEID', 'SALARY_FLOOR']\n",
    ")['VALUE'].rank(method='min', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "slate_data = slate_players.groupby(['DATE', 'SLATEID', 'NAME']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'L1P_COUNT': x['L1P_COUNT'].mean(),\n",
    "        'L1P_RANK': x['L1P_RANK'].mean(),\n",
    "        'L1P_SB_COUNT': x['L1P_SB_COUNT'].mean(),\n",
    "        'L1P_SB_RANK': x['L1P_SB_RANK'].mean(),\n",
    "        'L2P_COUNT': x['L2P_COUNT'].mean(),\n",
    "        'L2P_RANK': x['L2P_RANK'].mean(),\n",
    "        'L2P_SB_COUNT': x['L2P_SB_COUNT'].mean(),\n",
    "        'L2P_SB_RANK': x['L2P_SB_RANK'].mean(),\n",
    "        'L3P_COUNT': x['L3P_COUNT'].mean(),\n",
    "        'L3P_RANK': x['L3P_RANK'].mean(),\n",
    "        'L3P_SB_COUNT': x['L3P_SB_COUNT'].mean(),\n",
    "        'L3P_SB_RANK': x['L3P_SB_RANK'].mean()\n",
    "    })\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressors.append('L1P_COUNT')\n",
    "regressors.append('L1P_RANK')\n",
    "regressors.append('L1P_SB_COUNT')\n",
    "regressors.append('L1P_SB_RANK')\n",
    "regressors.append('L2P_COUNT')\n",
    "regressors.append('L2P_RANK')\n",
    "regressors.append('L2P_SB_COUNT')\n",
    "regressors.append('L2P_SB_RANK')\n",
    "regressors.append('L3P_COUNT')\n",
    "regressors.append('L3P_RANK')\n",
    "regressors.append('L3P_SB_COUNT')\n",
    "regressors.append('L3P_SB_RANK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxscores['GP'] = 1\n",
    "boxscores = feature_creation.expanding_sum(df=boxscores, group_col_names=['SEASON', 'PLAYERID'], col_name='GP', new_col_name='COUNT_GP')\n",
    "regressors.append('COUNT_GP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Now that I have created all the features I found to be indicative of our dependant variable, I will prepare the data to be suitable for a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will merge the player, contest, and slate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ownership_data = ownership_data.merge(slate_data, on=['DATE', 'SLATEID', 'NAME'], how='inner')\n",
    "all_ownership_data = ownership_data.merge(boxscores, on=['DATE', 'NAME'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.min_rows', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "display(all_ownership_data[['DATE', 'SLATEID', 'NAME'] + regressors + categorical_regressors + [y]].dropna(subset=[y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I will do is handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = all_ownership_data[['DATE', 'SLATEID', 'NAME'] + regressors + [y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix['L1_DKFP'] = feature_matrix['L1_DKFP'].fillna(feature_matrix['AVG_DKFP'])\n",
    "feature_matrix['MA5_DKFP'] = feature_matrix['MA5_DKFP'].fillna(feature_matrix['AVG_DKFP'])\n",
    "\n",
    "feature_matrix['SALARY_CHANGE'] = feature_matrix['SALARY_CHANGE'].fillna(0)\n",
    "\n",
    "feature_matrix['STD_DKFP'] = feature_matrix['STD_DKFP'].fillna(0.35*feature_matrix['AVG_DKFP'])\n",
    "\n",
    "feature_matrix['L1_OWNERSHIP'] = feature_matrix['L1_OWNERSHIP'].fillna(feature_matrix['AVG_OWNERSHIP'])\n",
    "feature_matrix['MA5_OWNERSHIP'] = feature_matrix['MA5_OWNERSHIP'].fillna(feature_matrix['AVG_OWNERSHIP'])\n",
    "\n",
    "feature_matrix['AVG_DvP'] = feature_matrix['AVG_DvP'].fillna(0)\n",
    "\n",
    "feature_matrix['TOTAL'] = feature_matrix['TOTAL'].fillna(feature_matrix['TOTAL'].mean())\n",
    "feature_matrix['POINTSPREAD'] = feature_matrix['POINTSPREAD'].fillna(0)\n",
    "\n",
    "feature_matrix['L1P_SB_COUNT'] = feature_matrix['L1P_SB_COUNT'].fillna(0)\n",
    "feature_matrix['L2P_SB_COUNT'] = feature_matrix['L2P_SB_COUNT'].fillna(0)\n",
    "feature_matrix['L3P_SB_COUNT'] = feature_matrix['L3P_SB_COUNT'].fillna(0)\n",
    "\n",
    "# we can predict Y for a player as long as AVG_Y is not nan\n",
    "feature_matrix = feature_matrix.dropna(subset=['AVG_OWNERSHIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers = Helpers()\n",
    "plt.rcParams[\"figure.figsize\"] = (22, 10)\n",
    "helpers.visualize_dataframe(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Feature Selection\n",
    "Finally, it's time to build the model. I will iterate through various model types, and choose the model that yeilds the lowest average MSE after 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linear_models = [\n",
    "    linear_model.LinearRegression()\n",
    "#     linear_model.Ridge(),\n",
    "#     linear_model.ElasticNet()\n",
    "]\n",
    "\n",
    "neighbor_models = [\n",
    "#     neighbors.KNeighborsRegressor()\n",
    "]\n",
    "\n",
    "boosted_models = [\n",
    "    xgb.XGBRegressor(objective='reg:squarederror', eval_metric='mae', learning_rate=0.1, n_estimators=500, tree_method='hist'),\n",
    "    CatBoostRegressor(eval_metric='MAE', learning_rate=0.1, n_estimators=500, silent=True)\n",
    "    \n",
    "]\n",
    "\n",
    "neural_networks = [\n",
    "#     neural_network.MLPRegressor(max_iter=10000, early_stopping=True, random_state=42)\n",
    "]\n",
    "\n",
    "models = linear_models + neighbor_models + boosted_models + neural_networks\n",
    "\n",
    "transformer = {\n",
    "        'Function': lambda x: x,\n",
    "        'Inverse Function': lambda x: x,\n",
    "        'Name': 'None'\n",
    "}\n",
    "\n",
    "numeric_regressors = list(set(regressors) - set(categorical_regressors))\n",
    "numeric_columns = numeric_regressors + [y]\n",
    "\n",
    "min_score = float('inf')\n",
    "best_model = None\n",
    "for model in models:\n",
    "      \n",
    "    # transform numeric columns\n",
    "    temp_feature_matrix = feature_matrix.copy()\n",
    "    temp_feature_matrix[numeric_columns] = temp_feature_matrix[numeric_columns].apply(\n",
    "        transformer['Function'], axis=1\n",
    "    )\n",
    "\n",
    "    # feature selection\n",
    "    X = temp_feature_matrix[regressors]\n",
    "    Y = temp_feature_matrix[y]\n",
    "\n",
    "    if model in (neighbor_models + neural_networks):\n",
    "        selected_features = regressors.copy()\n",
    "    else:\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # importance\n",
    "        importance_selector = SelectFromModel(model, threshold='0.75*median')\n",
    "        importance_selector = importance_selector.fit(X, Y)\n",
    "        importance_support = importance_selector.get_support()\n",
    "\n",
    "        # rank\n",
    "        rank_selector = RFE(model)\n",
    "        rank_selector = rank_selector.fit(X, Y)\n",
    "        rank_support = rank_selector.ranking_\n",
    "        median_rank = np.median(rank_support)\n",
    "\n",
    "        selected_features = []\n",
    "        for feature, importance_flag, rank in zip(list(X.columns), importance_support, rank_support):\n",
    "            # select the feature if it's important, or has a low rank\n",
    "            if importance_flag or (rank <= median_rank):\n",
    "                selected_features.append(feature)\n",
    "\n",
    "    X = temp_feature_matrix[selected_features].values\n",
    "    Y = temp_feature_matrix[y].values\n",
    "\n",
    "    # cross validation\n",
    "    scores = []\n",
    "    cv = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "    for train, test in cv.split(X, Y):\n",
    "        prediction = model.fit(X[train], Y[train]).predict(X[test])\n",
    "        true = Y[test]\n",
    "\n",
    "        prediction = transformer['Inverse Function'](prediction)\n",
    "        true = transformer['Inverse Function'](true)\n",
    "\n",
    "        scores.append(metrics.mean_absolute_error(prediction, true))\n",
    "\n",
    "    mean_score = np.mean(scores)\n",
    "    print('\\nModel:', model)\n",
    "    print('Transformer:', transformer['Name'])\n",
    "    print('Selected Features:', selected_features)\n",
    "    print('Mean MAE:', mean_score)\n",
    "    if mean_score < min_score:\n",
    "        min_score = mean_score\n",
    "        best_model = (model, transformer['Name'], selected_features)\n",
    "\n",
    "print()\n",
    "print(best_model, min_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning\n",
    "Now, I will use Grid Search Cross Validation to find the CatBoostRegressor paramters that yeild the lowest average MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features =  ['AVG_DKFP', 'VALUE', 'L1_DKFP', 'MA5_DKFP', 'SALARY', 'SALARY_CHANGE', 'STD_DKFP',\n",
    "                      'START', 'NUM_POSITIONS', 'SG', 'C', 'PG', 'AVG_OWNERSHIP', 'L1_OWNERSHIP',\n",
    "                      'MA5_OWNERSHIP', 'AVG_DvP', 'TOTAL', 'POINTSPREAD', 'GAMECOUNT', 'L1P_COUNT',\n",
    "                      'L1P_RANK', 'L1P_SB_COUNT', 'L1P_SB_RANK', 'L2P_COUNT', 'L3P_COUNT', 'L3P_RANK',\n",
    "                      'L3P_SB_COUNT', 'L3P_SB_RANK', 'COUNT_GP']\n",
    "# selected_features = best_model[3]\n",
    "X = feature_matrix[selected_features]\n",
    "Y = feature_matrix[y]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "\n",
    "model_params = {\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'eta' : [0.01, 0.02, 0.05],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 0.9, 1],\n",
    "    'n_estimators': [2000],\n",
    "    'eval_metric': ['mae'],\n",
    "    'tree_method': ['hist']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=model_params, scoring='neg_mean_absolute_error', cv=2, n_jobs=-1)\n",
    "\n",
    "fit_params = {\n",
    "    \"early_stopping_rounds\": 25,\n",
    "    \"eval_set\": [(X_test, y_test)]\n",
    "}\n",
    "\n",
    "grid.fit(X_train, y_train, **fit_params)    \n",
    "\n",
    "print(\"\\n========================================================\")\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"========================================================\")    \n",
    "\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)\n",
    "\n",
    "print(\"\\n ========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Lastly, I will evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = grid.best_params_\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will visualize the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Importance': feature_importances, 'Feature': selected_features})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance')\n",
    "ax = sns.barplot(x=\"Feature\", y=\"Importance\", data=feature_importance_df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will calculate a few metrics to evaluate the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_avg = feature_matrix.loc[X_test.index, 'AVG_OWNERSHIP']\n",
    "\n",
    "# Kpi\n",
    "print(\"R2 (explained variance):\")\n",
    "print(\"Model:\", round(metrics.r2_score(y_test, y_hat), 3))\n",
    "print(\"Baseline:\", round(metrics.r2_score(y_test, y_avg), 3))\n",
    "\n",
    "print(\"\\nMean Absolute Error (Σ|y-pred|/n):\")\n",
    "print(\"Model:\", round(metrics.mean_absolute_error(y_test, y_hat), 6))\n",
    "print(\"Baseline:\", round(metrics.mean_absolute_error(y_test, y_avg), 6))\n",
    "\n",
    "print(\"\\nRoot Mean Squared Error (sqrt(Σ(y-pred)^2/n)):\")\n",
    "print(\"Model:\", round(np.sqrt(metrics.mean_squared_error(y_test, y_hat)), 6))\n",
    "print(\"Baseline:\", round(np.sqrt(metrics.mean_squared_error(y_test, y_avg)), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_hat\n",
    "\n",
    "# plot predicted vs true\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 10))\n",
    "sns.scatterplot(y_hat, y_test, ax=ax[0])\n",
    "abline_plot(intercept=0, slope=1, color=\"red\", ax=ax[0])\n",
    "ax[0].grid(True)\n",
    "ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")\n",
    "ax[0].legend()\n",
    "    \n",
    "# plot predicted vs residuals\n",
    "sns.scatterplot(y_hat, residuals, ax=ax[1])\n",
    "ax[1].grid(True)\n",
    "ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n",
    "ax[1].hlines(y=0, xmin=np.min(y_hat), xmax=np.max(y_hat))\n",
    "ax[1].legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "sns.distplot(residuals, color=\"red\", hist=True, kde=True, kde_kws={\"shade\":True}, ax=ax)\n",
    "ax.grid(True)\n",
    "ax.set(title=\"Residuals distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = shapiro(residuals)\n",
    "print(stat, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are normally distributed and not heteroskedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_matrix.loc[X_test.index, 'RESULT'] = y_hat\n",
    "display(feature_matrix.loc[X_test.index, ['DATE', 'NAME', 'GAMECOUNT', 'OWNERSHIP', 'RESULT']].sort_values(by=['DATE', 'NAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(contests.loc[(contests['SLATEID'] == '5ae4cbe2c074cf3dff32fa1b')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python39264bitvenvvenvc591ed29d13e45c2aa0d5bd3ede2ed76"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "73c239050cff4cff9f35b3fe2630d8bf34fb505b5bcd0ef06706c6b2c8c9ea61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
